import requests
from bs4 import BeautifulSoup
import pandas as pd

# 1. Definir o endere√ßo do alvo.

url_alvo = "https://www.cert.br/docs/"

print(f"Iniciando investiga√ß√£o para o alvo: {url_alvo}")

# 1.1 Uso de um bloco try-except para lidar com poss√≠veis erros de conex√£o.

try:
    # Enviamos o "agente" (requests.get) para a URL.
    # O timeout (em segundos) evita que nosso programa fique "preso" para sempre
    # se o site demorar muito para responder.

    resposta = requests.get(url_alvo, timeout=10)

    # Verificar o "status code" da resposta.
    # C√≥digo 200 significa √™xito!

    if resposta.status_code == 200:
        print("Sucesso! ‚úî O agente se conectou ao servidor.")

        # 1.2 Guardar o conte√∫do da p√°gina (HTML completo) em uma vari√°vel.
        html_pagina = resposta.text
        print("O c√≥digo HTML foi baixado com sucesso! ‚úî")
        print("-" * 30)
        # Linha de baixo üëá se caso queira ver parte do HTML da p√°gina. Apenas "descomentar".
        # print(html_pagina[:500])

        # 2 Entregar o HTML para a biblioteca BeautifulSoup analisar.

        sopa = BeautifulSoup(html_pagina, "html.parser")

        # 2.1 Pedir a "sopa" encontrar TODAS as divs com a classe
        # Aqui √© onde, dependendo do site alvo, precisaremos alterar conforme a estrutura HTML da p√°gina.

        publicacoes = sopa.find_all("div", class_="content-respostas")

        # 2.2 Armazenar estruturas numa lista.

        lista_publicacoes = []

        print(
            f"Encontradas {len(publicacoes)} publica√ß√µes na p√°gina. Iniciando extra√ß√£o... ‚ò¢"
        )

        # 3 Criar um loop para percorrer todas as publica√ß√µes.

        for pub in publicacoes:
            # Para cada "pub", procuramos as tags identificadoras.
            titulo_bruto = pub.find("h3", class_="title").text
            resumo_bruto = pub.find("p").text
            # Pequena altera√ß√£o na estrutura para o resultado visual ficar melhor.
            titulo_limpo = ' '.join(titulo_bruto.split())
            resumo_limpo = ' '.join(resumo_bruto.split())

            link_bruto = pub.find("a")["href"]
            # Pequena verifica√ß√£o de URL incompleta
            if link_bruto.startswith("/"):
                link_completo = f"https://www.cert.br{link_bruto}"
            else:
                link_completo = link_bruto

            # 3.1 Ap√≥s percorrer as informa√ß√µes tronco, organizaremos num dicion√°rio.

            info_pubs = {"titulo": titulo_limpo, "resumo": resumo_limpo, "link": link_completo}

            # 3.2 Por fim, adicionamos o dicion√°rio na lista principal.

            lista_publicacoes.append(info_pubs)

        # 4 Se tudo correr bem, exibir√° mensagem:

        print("Extra√ß√£o finalizada com sucesso!")
        print("-" * 30)

        # 5 Gerar o relat√≥rio estruturado dos resultados obtidos na extra√ß√£o de dados.

        print("\n--- üì∞ RELAT√ìRIO DE PUBLICA√á√ïES DE CIBERSEGURAN√áA üì∞ ---\n")

        # 5.1 Estrutura√ß√£o dos dados usando Pandas.
        # Convers√£o da lista em um DataFrame com ajustes para colunas ficarem sim√©tricas.

        df_pubs = pd.DataFrame(lista_publicacoes)
        pd.set_option("display.max_rows", 50)
        pd.set_option("display.max_columns", 5)
        pd.set_option("display.width", 150)
        pd.set_option("display.max_colwidth", 100)

        # 5.2 Exibir o DataFrame.

        print(df_pubs)

        # 5.3 - EXTRA - Se for necess√°rio salvar em um arquivo CSV -> descomentar linhas abaixo.

        # df_pubs.to_csv('publicacoes_cert.csv', index=False)
        # print("\nRelat√≥rio salvo em 'publicacoes_cert.csv'")

    else:
        print(f"Erro! ‚ùå O servidor respondeu com o c√≥digo: {resposta.status_code}")

except requests.exceptions.RequestException as e:
    print(f"O agente n√£o conseguiu se conectar com o alvo. ‚ùå")
    print(f"Erro: {e}")
